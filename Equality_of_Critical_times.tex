\section{Equality of the Critical Times} \label{sec: ECT}

In this section, we will prove that the critical time $t_\mathrm{crit}$ for the graph process, introduced in Section \ref{sec: coupling_to_random_graph}, coincides with the gelation time for the limiting equation, defined in Section \ref{sec:SE} as the time at which mass escapes to infinity. 
\begin{lemma}\label{lemma: connect critical times} Let $\mu_0$ be a probability measure on $S$ satisfying (\textbf{A1-4}). Let $(\mu_t)_{t\ge 0}$ be the solution to (\ref{eq: E+G}) starting at $\mu_0$, with associated mass $M_t$ and energy $E_t$ of the gel; let $t_\mathrm{gel}$ be given by \begin{equation}
    t_\mathrm{gel}=\inf\{t\ge 0: M_t>0\}.
\end{equation} Let $(G^N_t)$ be a random graph process for the same base measure $m$ on $\mathbb{R}^d$, and let $t_\mathrm{crit}$ be the critical time calculated in Lemma \ref{lemma: computation of tcrit}. Then $t_\mathrm{crit}=t_\mathrm{gel}$. \end{lemma}
The proof is based on the following weak version of the convergence of the gel in Theorem \ref{thrm: convergence of stochastic coagulent}, and may be taken as preliminary reading for the more detailed arguments in Section \ref{sec: COG}. \begin{lemma} \label{lemma: WCOG} Fix $t>0$, and write $M^N_t$ for the scaled mass of the largest component of $G^N_t$: \begin{equation} M^N_t:=\frac{1}{N}\#C_1(G^N_t). \end{equation} Then $M^N_t\rightarrow M_t$ in probability. \end{lemma} We first show that Lemma \ref{lemma: WCOG} implies Lemma \ref{lemma: connect critical times}; the remainder of this section is dedicated to the proof of Lemma \ref{lemma: WCOG}. \begin{proof}[Proof of Lemma \ref{lemma: connect critical times}] Firstly, suppose $t>t_\mathrm{gel}$. Then, by Lemma \ref{lemma: connecting mu-epsilon and mu}, $M_t>0$, and so the convergence $\frac{1}{N}C_1(G^N_t)=M^N_t\rightarrow M_t$ in probability implies the existence of a constant $c=c(t)>0$ such that $C_1(G^N_t)\ge cN$ with high probability. From the definition of the phase transition in Theorem \ref{thrm: RG1}, this implies that $G^N_t$ is supercritical, and so $t>t_\mathrm{crit}$. \medskip \\ Conversely, if $t< t_\mathrm{gel}$, then $M_t=0$ by definition, and the convergence $\frac{1}{N}C_1(G^N_t)$ implies that $t\le t_\mathrm{crit}$. Together, these imply that $t_\mathrm{gel}=t_\mathrm{crit}.$  \end{proof} 
\subsection{Preparatory Lemmas} We begin with some preparatory lemmas before we turn to the proof of \ref{lemma: WCOG}. Firstly, we  bound the size of the largest cluster \emph{below}, even in the cases where there is no giant component. In conjunction with Theorem \ref{thrm: RG2}, this allows to extract information about the largest cluster by considering all clusters of size at least $\xi_N$, for some suitably slow-growing sequence $\xi_N$.
\begin{lemma}\label{lemma: lower bound on largest cluster} Fix $t>0$ and $r\in \mathbb{N}$, and let $G^N_t$ be the random graph process for a base measure $m$ satisfying (\textbf{A1-4}). Then \begin{equation}
    \mathbb{P}(C_1(G^N_t)\leq r)\rightarrow 0.
\end{equation} \end{lemma} \begin{proof} From (\textbf{A1}, \textbf{A4}), it follows that the measure $m$ is not a point mass, and we can therefore find open sets $V, W$ of positive $m$- measure such that $\inf_{v \in V, w \in W} |v-w|>0$. Define \begin{equation} I_1=\{i \in \{1,2,...,N\}: v_i \in V\}; \hspace{1cm} N_1=\#I_1
\end{equation} and similarly, $I_2, N_2$, with $V$ replaced by $W$. By weak convergence and openness of $V$ and $W$, $N_1\geq cN$ and $N_2\geq cN$ with high probability, for some constant $c>0$. Now, for all $i\in I_1, j\in I_2$ the edge $e=(ij)$ is present in $G^N_t$ with probability \begin{equation}
    \mathbb{P}(e \text{ present in }G^N_t) \geq \left(1-\exp\left(-\frac{\epsilon}{N}\right)\right) \geq \frac{\delta}{N}
\end{equation} for all $N$ large enough, for some positive $\epsilon(t, V, W)>0$ and $\delta=\delta(t, V, W)>0$. \medskip \\ Therefore, we can couple a random bipartite graph $H^N$, with vertex sets size $cN$ and edge probability $\frac{\delta}{N}$, such that $H^N \subset G^N_t$ with high probability. It is straightforward to see that the maximum degree $\Delta(H^N)\rightarrow \infty$ in probability, which implies the claim.   \end{proof}

For the proof of of Lemma \ref{lemma: connect critical times}, and later Theorem \ref{thrm: convergence of stochastic coagulent}, we will wish to study the convergence of integrals $\langle (\pi_n+\pi_e)f, \mu^N_t\rangle$, for bounded continuous functions $f$ with non-compact support. However, the convergence result Lemma \ref{lemma: local uniform convergence of stochastic coagulent} only gives us information when the support of $f$ is compact. Our second preparatory lemma allows us to approximate the integrals $\langle (\pi_n+\pi_e)f, \mu^N_t\rangle$ for functions \emph{whose support is bounded in the $\pi_n$-direction}. \begin{lemma}\label{lemma: etar} There exists a sequence $\eta_r \rightarrow \infty$ such that \begin{equation}
    \beta_n(r)=\sup_{N\geq 1} \hspace{0.1cm} \mathbb{E}\left[\sup_{t\geq 0}\langle \pi_n 1[\pi_e(x)>\eta_r, \pi_n(x)< r], \mu^N_t\rangle \right]\rightarrow 0;
\end{equation} \begin{equation}
    \beta_e(r)= \sup_{N\geq 1} \hspace{0.1cm} \mathbb{E}\left[\sup_{t\geq 0}\langle \pi_e 1[\pi_e(x)>\eta_r, \pi_n(x)< r], \mu^N_t\rangle\right] \rightarrow 0. 
\end{equation} \end{lemma} \begin{proof} Let $\mu^N_t$ be a stochastic coagulant coupled to a random graphs process $G^N_t$. Using Cauchy-Schwarz, \begin{equation} \begin{split}
   \langle \pi_e 1[\pi_e(x)>\eta_r, \pi_n(x)< r], \mu^N_t\rangle & = \frac{1}{N} \sum_{\substack{j\geq 1: C_j(G^N_t)\le r \\ E(\mathcal{C}_j(G^N_t))>\eta_r}} \hspace{0.2cm} \sum_{i\in C_j(G^N_t)} \frac{1}{2}|v_i|^2  \\ & \leq \left(\frac{1}{N} \sum_{\substack{j\geq 1: C_j(G^N_t)\le r \\[1ex] E(\mathcal{C}_j(G^N_t))>\eta_r}} C_j(G^N_t)\right)^\frac{1}{2}\left(\frac{1}{N}\sum_{i=1}^N \frac{1}{4}|v_i|^4\right)^\frac{1}{2}  \\[2ex] & = \langle \pi_n 1[\pi_e(x)>\eta_r, \pi_n(x)< r], \mu^N_t\rangle^\frac{1}{2}\left(\frac{1}{N}\sum_{i=1}^N \frac{1}{4}|v_i|^4\right)^\frac{1}{2}.
\end{split} \end{equation} The second factor is constant in time, and bounded in $L^2$ by (\textbf{A4}), and so it is sufficient to prove the case for $\pi_n$. \medskip \\ Choose $\eta_r\rightarrow \infty$ quickly enough that \begin{equation} \label{eq: choice of etar}
    r\int_{\mathbb{R}^d}1\left[\frac{1}{2}|v|^2>\frac{\eta_r}{r}\right]m(dv)\rightarrow 0.
\end{equation}We observe that \begin{equation}
    \sup_{t\geq 0} \hspace{0.1cm} \langle \pi_n 1[\pi_e(x)>\eta_r, \pi_n(x)< r], \mu^N_t\rangle \leq \frac{1}{N}\sum_{i=1}^N 1_{A_i}  
\end{equation} where \begin{equation}
    A_i=\left\{\exists \hspace{0.1cm} t: C(i, G^N_t)< r, \hspace{0.2cm} E\left(\mathcal{C}(i, G^N_t)\right)> \eta_r \right\}.
\end{equation} Let \begin{equation}
    \mathcal{X}_{N,r}=\left\{i \in \{1,2,..,N\}: \hspace{0.1cm} \frac{1}{2}|v_i|^2>\frac{\eta_r}{r}\right\}; \hspace{1cm}X_{N,r}=\#\mathcal{X}_{N,r}.
\end{equation} For $i\in \mathcal{X}_{N,r}$, let $t_{i}$ be the first time that the cluster $\mathcal{C}(i, G^N_t)$ is of size $r$, and define \begin{equation}
    \mathcal{C}^*(i,r,N)=\mathcal{C}(i, G^N_{t_i});\hspace{1cm} \mathcal{C}^*(r,N)=\bigcup_{i\in \mathcal{X}_{N,r}} \mathcal{C}^*(i,r,N).
\end{equation} Then we bound $\#\mathcal{C}^*(r,N)\le r X_{N,R}$.  \medskip \\ With this notation, we can prove the claimed convergence. Fix $i$, and suppose that, for some time $t\geq 0$, $C(i, G^N_t)< r$ and $E(\mathcal{C}(i, G^N_t))> \eta_r.$ Then there exists some $j \in \mathcal{C}(i, G^N_t)$ with $\frac{1}{2}|v_j|^2 >\frac{\eta_r}{r}$, and so $j\in \mathcal{X}_{N,r}$. Since $C(j, C^N_t)=C(i, G^N_t)< r$, we must also have $t\le t_j$ and so $i\in\mathcal{C}(j,G^N_t) \subset \mathcal{C}^*(r,N).$ Therefore, \begin{equation}
    \sup_{t\geq 0} \hspace{0.1cm} \langle \pi_n 1[\pi_e(x)>\eta_r, \pi_n(x)\le r], \mu^N_t\rangle \le \frac{1}{N}\sum_{i=1}^N 1_{A_i} \le \frac{1}{N}\#\mathcal{C}(r,N) \le \frac{r}{N}X_{N,r}.
\end{equation} Taking expectations, and using that the initial velocities are sampled independently from $m$, \begin{equation} \begin{split}
    \mathbb{E}\left[\sup_{t\geq 0} \hspace{0.1cm} \langle \pi_n 1[\pi_e(x)>\eta_r, \pi_n(x)\le r], \mu^N_t\rangle\right] \le \frac{r}{N} \mathbb{E} X_{N,r}  = r\int_{\mathbb{R}^d} 1\left[\frac{1}{2}|v|^2>\frac{\eta_r}{r}\right]m(dv) \rightarrow 0. 
\end{split} \end{equation}\end{proof}
\subsection{Proof of Lemma \ref{lemma: WCOG}} Using the two preparatory lemmas developed above, we now prove Lemma \ref{lemma: WCOG}. 
\begin{proof}[Proof of Lemma \ref{lemma: WCOG}]  Throughout, we let $(\mu^N_t)_{t\geq 0}$ be a stochastic coagulant coupled to a random graphs process $(G^N_t)_{t\geq 0}$, as described in Section \ref{sec: coupling_to_random_graph}. We write $(v_i)_{i=1}^N$ for the velocities associated to the graph vertexes. \medskip \\ Fix $t\ge 0$, and let $\xi_N$ be a sequence, to be constructed later, such that \begin{equation}\label{eq: choice of xiN for WCOG}
       \xi_N\rightarrow \infty; \hspace{1cm} \frac{\xi_N}{N}\rightarrow 0; \hspace{1cm}\mathbb{P}(C_1(G^N_t)\geq \xi_N)\rightarrow 1.
   \end{equation}  We now construct `bump functions' as follows.  For any $r \in \mathbb{N}$, let $\eta_r$ be as in Lemma \ref{lemma: etar} and let $X_r$ be the set \begin{equation}
       \{x \in S: \pi_n(x)< r, |\pi_p(x)|\leq \sqrt{2r\eta_r}, \pi_e(x)\leq \eta_r\}.
   \end{equation} Let $\widetilde{g}_r$ be the indicator $\widetilde{g}_r=1[\pi_n(x)< r]$, and construct a continuous, compactly supported function $\widetilde{f}_r$ such that \begin{equation}
      0\leq \widetilde{f}_r\leq 1;\hspace{1cm} \widetilde{f}_r=1 \hspace{0.1cm} \text{ on } X_r;\hspace{1cm} \widetilde{f}_r(x)=0 \hspace{0.1cm} \text{ if } \pi_n(x)\ge r.
   \end{equation} We define $f_N=\widetilde{f}_{\xi_N}$ and $g_N=\widetilde{g}_{\xi_N}$.  We now decompose the difference $M^N_t-M_t:$ \begin{equation}\label{eq: decomposition of erorr in WCOG}\begin{split} M^N_t-M_t &= \underbrace{(1-M_t-\langle \pi_n f_N, \mu_t\rangle)}_{:=\mathcal{T}^1_N} + \underbrace{\langle \pi_n f_N, \mu_t-\mu^N_t\rangle}_{:=\mathcal{T}^2_N} + \underbrace{\langle \pi_n (f_N-g_N), \mu^N_t\rangle}_{:=\mathcal{T}^3_N} +\underbrace{(
   \langle \pi_n g_N, \mu^N_t\rangle - (1-M^N_t))}_{:=\mathcal{T}^4_N}\end{split} \end{equation} We estimate the errors $\mathcal{T}^i_N$, $i=1,3,4.$ The remaining term $\mathcal{T}^2_N$ will be dealt with separately, and requires careful construction of the sequence $\xi_N$. \paragraph{1. Estimate on $\mathcal{T}^1_N$.} Let $h_N=1_{X_{\xi_N}}$, so that $h_N \le f_N \le 1$. As $N\rightarrow \infty$, $\pi_n h_N \uparrow \pi_n$, and so by monotone convergence, \begin{equation}
       \langle \pi_n h_N, \mu_t\rangle \uparrow \langle \pi_n, \mu_t\rangle =1-M_t.
   \end{equation} This implies immediately that the (nonrandom) error $\mathcal{T}^1_N \rightarrow 0$.
   \paragraph{2. Estimate on $\mathcal{T}^3_N$.} From the definitions of $f_N, g_N$, we observe that \begin{equation}
       |\mathcal{T}^3_N(t)|=\langle \pi_n(g_N-f_N), \mu^N_t\rangle \le  \langle \pi_n 1[\pi_n(x)<\xi_N, \pi_e(x)>\eta_{\xi_N}], \mu^N_t\rangle.
   \end{equation} Therefore, in the notation of Lemma \ref{lemma: etar}, \begin{equation}
       \mathbb{E}\left[|\mathcal{T}^3_N(t)|\right] \leq \beta_n(\xi_N).
   \end{equation} By construction of $\eta_r$, and since $\xi_N \rightarrow \infty$, it follows that $\mathbb{E}[ |\mathcal{T}^3_N(t)|] \rightarrow 0,$ which implies convergence to $0$ in probability.
       \paragraph{3. Estimate on $\mathcal{T}^4_N$.} By the choice (\ref{eq: choice of xiN for WCOG}) of $\xi_N$, we have that $\mathbb{P}( C_1(G^N_t)\geq \xi_N)\rightarrow 1.$ On this event, we have the equality \begin{equation}
           \begin{split}
               \langle \pi_n g_N, \mu^N_t\rangle &=\langle \pi_n, \mu^N_t\rangle - \langle \pi_n 1[\pi_n\geq \xi_N], \mu^N_t\rangle \\[2ex] & = 1-\frac{1}{N}\sum_{j\geq 1: C_j(G^N_t)\ge \xi_N} C_j(G^N_t) \\[2ex] & = 1-M^N_t-\frac{1}{N}\sum_{j\ge 2:C_j(G^N_t)\ge \xi_N} C_j(G^N_t) 
           \end{split} 
       \end{equation} where $(G^N_t)_{t\geq 0}$ is the random graph process coupled to the stochastic coagulant. Therefore, with high probability, \begin{equation} \mathcal{T}^4_N(t) = \frac{1}{N} \sum_{j\ge 2:C_j(G^N_t)\ge \xi_N} C_j(G^N_t) \end{equation} is the mass of the anomalous clusters, which converges to $0$ in probability, by Theorem \ref{thrm: RG2}.
       \paragraph{4. Construction of $\xi_N$, and convergence of $\mathcal{T}^2_N$.} It remains to show how a sequence $\xi_N$ can be constructed such that $\mathcal{T}^2_N \rightarrow 0$ in probability. Let $A^1_{r,N}, A^2_{r,N}$ be the events \begin{equation} \label{eq: definition of A1rn for WCOG}
       A^1_{r,N}=\left\{ |\langle \pi_n f_r, \mu^N_t-\mu_t\rangle|<\frac{1}{r}\right\}; \hspace{1cm}
       A^2_{r,N}=\left\{C_1(G^N_{t_\mathrm{gel}}) \geq r\right\}.
   \end{equation} Then, as $N\rightarrow \infty$, both $\mathbb{P}(A^1_{r,N}), \mathbb{P}(A^2_{r,N}) \rightarrow 1$, by Lemma \ref{lemma: local uniform convergence of stochastic coagulent} and Lemma \ref{lemma: lower bound on largest cluster}. We now define $N_r$ inductively for $r\geq 1$ by setting $N_1=1$ and \begin{equation}
       \label{eq: recursive definition of Nr for WCOG} N_{r+1}=\min\left\{n \in \mathbb{N}: n\geq \max((r+1)^2, N_r+1),  \mathbb{P}(A^2_{r+1,N})>\frac{r}{r+1},  \mathbb{P}(A^3_{r+1,N})>\frac{r}{r+1} \hspace{0.1cm} \mathrm{for all }N\geq n. \right\}
   \end{equation} Now, we set $\xi_N=r$ for $N\in [N_r, N_{r+1})\cap\mathbb{N}.$ It follows that $\xi_N \rightarrow \infty$ and $\xi_N\leq \sqrt{N}\ll N$, and \begin{equation}
       \mathbb{P}\left(C_1(G^N_{t_\mathrm{gel}}))\geq \xi_N\right)\ge 1-\frac{1}{\xi_N} \rightarrow 1. 
   \end{equation} Therefore, $\xi_N$ satisfies the requirements (\ref{eq: choice of xiN for WCOG}) above. Moreover, \begin{equation}
       \mathbb{P}\left(|\mathcal{T}^2_N| <\frac{1}{\xi_N}\right) \ge \mathbb{P}\left(A^1_{\xi_N,N}\right) > 1-\frac{1}{\xi_N}\rightarrow 1
   \end{equation} and so, with this choice of $\xi_N$, $\mathcal{T}^2_N \rightarrow 0$ in probability. Since we have now dealt with every term appearing in the decomposition (\ref{eq: decomposition of erorr in WCOG}), it follows that $M^N_t\rightarrow M_t$ in probability, as claimed. \end{proof} 